# CERN Multimodal RAG System

End-to-end **PDF ingestion + multimodal extraction + Retrieval-Augmented Generation (RAG)** system
for CERN Yellow Reports and radiation-materials datasets (MaxRAD / Imhotep).

The pipeline:
- extracts **text, tables, figures/graphs, and captions** from scientific PDFs,
- builds a **FAISS + Sentence-Transformers** semantic index over text,
- links text chunks to **images and tables** via captions,
- exposes an interactive **Streamlit QA app** where you can ask questions and request figures/tables.

> This repository is designed to run both **locally** and on **Google Colab** with GPU.

---

## Features

- ğŸ” **PDF ingestion + parsing**
  - Uses `PyMuPDF` / `fitz`, `camelot` / `tabula`, and OpenCV-style heuristics.
  - Extracts:
    - raw page text
    - table structures
    - figure/graph crops
    - image captions and local context

- ğŸ§  **Semantic RAG**
  - Sentence-Transformers encoder (all-MiniLM or similar).
  - FAISS index on normalized embeddings.
  - Simple re-ranking and metadata-rich retrieval results.

- ğŸ–¼ **Multimodal grounding**
  - Text chunks are linked to:
    - table metadata + CSV content
    - image / graph crops
    - captions around each figure
  - QA answers can surface both relevant text and visuals.

- ğŸ’» **Streamlit demo app**
  - Upload or select a PDF from the `data/` directory.
  - Run the full extraction pipeline from the UI.
  - Build / rebuild FAISS index.
  - Ask questions like:
    - *"Show me graphs related to tensile strength under irradiation"*
    - *"Which table summarizes material composition for alloy X?"*

---

## Repository structure



cern-multimodel-rag/
â”œâ”€â”€ app/
â”‚ â””â”€â”€ streamlit_app.py # ChatGPT-like frontend
â”œâ”€â”€ extraction/
â”‚ â”œâ”€â”€ extract_text.py
â”‚ â”œâ”€â”€ extract_images.py
â”‚ â”œâ”€â”€ extract_tables.py
â”‚ â”œâ”€â”€ extract_graphs.py
â”‚ â”œâ”€â”€ caption_images.py
â”‚ â”œâ”€â”€ build_metadata.py
â”‚ â””â”€â”€ pipeline.py # orchestrates extraction
â”œâ”€â”€ rag_pipeline.py # Qdrant Cloud + embeddings
â”œâ”€â”€ data/
â”‚ â””â”€â”€ CERN_Report.pdf # default PDF (optional)
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ .gitkeep # extraction results stored here
â”œâ”€â”€ docker-compose.yml # Docling + MinIO + Streamlit
â”œâ”€â”€ Dockerfile # Streamlit backend container
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

> Note: The `data/` folder is intentionally **empty** in the repository.
> You must provide your own PDFs (any name is fine).

---
# ğŸ”¬ CERN Multimodal RAG System
A full end-to-end, multimodal Retrieval-Augmented Generation (RAG) system for analyzing CERN Yellow Reports using:

- **Docling OCR** (Docker)
- **MinIO (S3 storage)**
- **Qdrant Cloud** (vector database)
- **Sentence-Transformers** (embeddings)
- **Streamlit Chat UI** (ChatGPT-like)
- **BLIP captions** for figures/graphs

This system extracts text, tables, images, figures & graphs from a PDF and enables semantic question-answering with sources.

---

## ğŸ“ Project Structure

